{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLdc6uY_usEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57629890-b2e9-4594-f038-f18d3f09e74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Dataset_bsl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "!ls '/content/gdrive/MyDrive/Datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrUGOmbIUIpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77a56be-daab-4722-91a8-e415dc14e2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.61.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.1 sounddevice-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install memory-profiler\n",
        "!pip install mediapipe opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p26NE554UC8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import glob\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mediapipe as mp\n",
        "import uuid\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "667lvwjNcNO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4euwVx9UMDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf45cd55-cff9-471c-94ef-78a6f6a66d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 "
          ]
        }
      ],
      "source": [
        "class_list=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37']\n",
        "\n",
        "\n",
        "dataset_path = '/content/gdrive/MyDrive/Datasets/Dataset_bsl/RESIZED_TESTING_DATA'\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for i in class_list:\n",
        "    for folder_name in os.listdir(dataset_path):\n",
        "        if folder_name == i:\n",
        "            folder_path = os.path.join(dataset_path, folder_name)\n",
        "            image_paths.append(folder_path)\n",
        "\n",
        "for i in image_paths:\n",
        "    print(i.rsplit('/', 1)[-1], end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWCRSwePUrdp"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvs8QOHZWgV5"
      },
      "outputs": [],
      "source": [
        "x = np.load('/content/gdrive/MyDrive/Datasets/Dataset_bsl/RESIZED_TESTING_DATA/test_o_img.npy')\n",
        "y = np.load('/content/gdrive/MyDrive/Datasets/Dataset_bsl/RESIZED_TESTING_DATA/test_o_lbl.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLvm8t4CYBLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1e865f-1576-4ccd-fe1e-9639a0009a86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK53Ha8aeNkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9243292e-3dcf-4013-e7c0-d12fe312a696"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53fP_OvOWyy5"
      },
      "source": [
        "## MODEL-1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxVgna1JWrBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a64b313-d28d-4847-f6da-71d1fa65a5e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_img_model1 = []\n",
        "test_label_model1 = y\n",
        "\n",
        "for i in range(len(x)):\n",
        "    image = x[i]\n",
        "    image = cv2.resize(image, (64,64), interpolation = cv2.INTER_AREA)\n",
        "    image = image.astype('float32')\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "    image = (image - min_val) / (max_val - min_val)\n",
        "    test_img_model1.append(image)\n",
        "\n",
        "test_img_model1 = np.array(test_img_model1)\n",
        "test_label_model1 = np.array(test_label_model1)\n",
        "\n",
        "test_img_model1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXPtAmkyYEph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "50d0882a-cba6-49fb-c6a2-e0c2542647fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F87A9E96140>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "cv2_imshow(test_img_model1[123]*255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l5TZQBeYS-x"
      },
      "source": [
        "## MODEL-2 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed7n0VwHYSPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b7a27f-eaac-48fb-a8b8-11d3088b0b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "test_img_model2 = []\n",
        "test_label_model2 = y\n",
        "\n",
        "\n",
        "for i in range(len(x)):\n",
        "    frame = x[i]\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    # Initialize MediaPipe Hands\n",
        "    mp_hands = mp.solutions.hands\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    with mp_hands.Hands(min_detection_confidence=0.2, min_tracking_confidence=0.5) as hands:\n",
        "        # BGR 2 RGB\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to uint8\n",
        "        image = image.astype(np.uint8)\n",
        "\n",
        "        # Set flag\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        # Detections\n",
        "        results = hands.process(image)\n",
        "\n",
        "        # Set flag to true\n",
        "        image.flags.writeable = True\n",
        "\n",
        "        # Create a black background image\n",
        "        image = np.zeros_like(image)\n",
        "\n",
        "        # Rendering results\n",
        "        if results.multi_hand_landmarks:\n",
        "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
        "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
        "                                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
        "                                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
        "                                        )\n",
        "        image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "        test_img_model2.append(image)\n",
        "\n",
        "test_img_model2 = np.array(test_img_model2)\n",
        "test_label_model2 = np.array(test_label_model2)\n",
        "\n",
        "test_img_model2.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTAbwb4ka7SB"
      },
      "source": [
        "## MODEL-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfckbwwna6wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776cb889-cb64-4866-bbf4-f7b23ad06ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crop not possible, index: 6\n",
            "label 34\n",
            "Crop not possible, index: 14\n",
            "label 34\n",
            "Crop not possible, index: 19\n",
            "label 34\n",
            "Crop not possible, index: 32\n",
            "label 34\n",
            "Crop not possible, index: 33\n",
            "label 34\n",
            "Crop not possible, index: 82\n",
            "label 37\n",
            "Crop not possible, index: 88\n",
            "label 37\n",
            "Crop not possible, index: 96\n",
            "label 37\n",
            "Crop not possible, index: 100\n",
            "label 37\n",
            "Crop not possible, index: 106\n",
            "label 37\n",
            "Crop not possible, index: 107\n",
            "label 37\n",
            "Crop not possible, index: 111\n",
            "label 37\n",
            "Crop not possible, index: 115\n",
            "label 37\n",
            "Crop not possible, index: 117\n",
            "label 37\n",
            "Crop not possible, index: 118\n",
            "label 37\n",
            "Crop not possible, index: 118\n",
            "label 37\n",
            "Crop not possible, index: 127\n",
            "label 6\n",
            "Crop not possible, index: 186\n",
            "label 35\n",
            "Crop not possible, index: 283\n",
            "label 4\n",
            "Crop not possible, index: 365\n",
            "label 36\n",
            "Crop not possible, index: 442\n",
            "label 30\n",
            "Crop not possible, index: 447\n",
            "label 30\n",
            "Crop not possible, index: 447\n",
            "label 30\n",
            "Crop not possible, index: 476\n",
            "label 30\n",
            "Crop not possible, index: 567\n",
            "label 32\n",
            "Crop not possible, index: 570\n",
            "label 32\n",
            "Crop not possible, index: 579\n",
            "label 32\n",
            "Crop not possible, index: 598\n",
            "label 32\n",
            "Crop not possible, index: 599\n",
            "label 32\n",
            "Crop not possible, index: 661\n",
            "label 29\n",
            "Crop not possible, index: 801\n",
            "label 24\n",
            "Crop not possible, index: 807\n",
            "label 24\n",
            "Crop not possible, index: 811\n",
            "label 24\n",
            "Crop not possible, index: 828\n",
            "label 24\n",
            "Crop not possible, index: 834\n",
            "label 24\n",
            "Crop not possible, index: 931\n",
            "label 15\n",
            "Crop not possible, index: 1000\n",
            "label 17\n",
            "Crop not possible, index: 1021\n",
            "label 17\n",
            "Crop not possible, index: 1028\n",
            "label 17\n",
            "Crop not possible, index: 1042\n",
            "label 18\n",
            "Crop not possible, index: 1066\n",
            "label 18\n",
            "Crop not possible, index: 1078\n",
            "label 18\n",
            "Crop not possible, index: 1078\n",
            "label 18\n",
            "Crop not possible, index: 1096\n",
            "label 20\n",
            "Crop not possible, index: 1149\n",
            "label 22\n",
            "Crop not possible, index: 1192\n",
            "label 16\n",
            "Crop not possible, index: 1199\n",
            "label 16\n",
            "Crop not possible, index: 1213\n",
            "label 21\n",
            "Crop not possible, index: 1268\n",
            "label 14\n",
            "Crop not possible, index: 1271\n",
            "label 14\n",
            "Crop not possible, index: 1276\n",
            "label 14\n",
            "Crop not possible, index: 1278\n",
            "label 14\n",
            "Crop not possible, index: 1284\n",
            "label 13\n",
            "Crop not possible, index: 1309\n",
            "label 13\n",
            "Crop not possible, index: 1309\n",
            "label 13\n",
            "Crop not possible, index: 1321\n",
            "label 12\n",
            "Crop not possible, index: 1328\n",
            "label 12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "test_img_model3 = []\n",
        "test_label_model3 = y\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in range(len(x)):\n",
        "    image = x[i]\n",
        "    # backup = image\n",
        "    frame = image\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "\n",
        "    # Initialize MediaPipe Hands\n",
        "    mp_hands = mp.solutions.hands\n",
        "\n",
        "    # Number of pixels to increase the bounding box size\n",
        "    padding = 0.15\n",
        "\n",
        "    with mp_hands.Hands(min_detection_confidence=0, min_tracking_confidence=0.5) as hands:\n",
        "        # BGR 2 RGB\n",
        "        frame = frame.astype(np.uint8)\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Set flag\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        image = image.astype(np.uint8)\n",
        "        # Detections\n",
        "        results = hands.process(image)\n",
        "\n",
        "        # Set flag to true\n",
        "        image.flags.writeable = True\n",
        "\n",
        "        # RGB 2 BGR\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        backup = image\n",
        "        # Detections\n",
        "        if results.multi_hand_landmarks:\n",
        "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
        "                # Get the bounding box coordinates\n",
        "                xcord = [landmark.x for landmark in hand.landmark]\n",
        "                ycord = [landmark.y for landmark in hand.landmark]\n",
        "\n",
        "                xmin, xmax = min(xcord), max(xcord)\n",
        "                ymin, ymax = min(ycord), max(ycord)\n",
        "\n",
        "                # Increase the bounding box size\n",
        "                xmin -= padding\n",
        "                xmax += padding\n",
        "                ymin -= padding\n",
        "                ymax += padding\n",
        "\n",
        "                # Convert the bounding box coordinates to pixel values\n",
        "                xmin_pixel = int(xmin * image.shape[1])\n",
        "                xmax_pixel = int(xmax * image.shape[1])\n",
        "                ymin_pixel = int(ymin * image.shape[0])\n",
        "                ymax_pixel = int(ymax * image.shape[0])\n",
        "\n",
        "                # Check if resized image has valid data\n",
        "                if xmin_pixel<0 or xmax_pixel<0 or ymin_pixel<0 or ymax_pixel<0:\n",
        "                    # Use the original image if the resized image has invalid data\n",
        "                    image_cropped = backup\n",
        "                    print('Crop not possible, index:', i)\n",
        "                    print('label', y[i])\n",
        "\n",
        "                else:\n",
        "                    # Crop the region within the bounding box\n",
        "                    image_cropped = image[ymin_pixel:ymax_pixel, xmin_pixel:xmax_pixel]\n",
        "\n",
        "                # Resize the cropped image to 224x224\n",
        "                image_resized = cv2.resize(image_cropped, (224, 224))\n",
        "\n",
        "                # Perform edge detection\n",
        "                blur1 = image_resized\n",
        "                canny1 = cv2.Canny(blur1, threshold1=140, threshold2=80)\n",
        "\n",
        "                image_processed = cv2.resize(canny1, (64, 64))\n",
        "                image_processed = cv2.cvtColor(image_processed, cv2.COLOR_GRAY2RGB)\n",
        "                image_processed = image_processed.astype('float32')\n",
        "        # else:\n",
        "        #     image_resized = cv2.resize(backup, (224, 224))\n",
        "\n",
        "        #     # Perform edge detection\n",
        "        #     blur1 = image_resized\n",
        "        #     canny1 = cv2.Canny(blur1, threshold1=140, threshold2=80)\n",
        "\n",
        "        #     image_processed = cv2.resize(canny1, (64, 64))\n",
        "        #     image_processed = cv2.cvtColor(image_processed, cv2.COLOR_GRAY2RGB)\n",
        "        #     image_processed = image_processed.astype('float32')\n",
        "\n",
        "    test_img_model3.append(image_processed)\n",
        "\n",
        "test_img_model3 = np.array(test_img_model3)\n",
        "test_label_model3 = np.array(test_label_model3)\n",
        "\n",
        "test_img_model3.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "_Mly5a-hYSNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24d3afa-75fd-4036-ead4-7cf380e4786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b0zcWnkc2Op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "5c25ce46-bb54-46b6-e21f-8e736d54fb8d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F87A9F48160>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAC3ElEQVR4nO1a0ZHjIAzVXQWkg5RAOiAlpIKwHZAKUgIleFIJTgW4hHSAO/B96IZjgGCMbey58fvatQE9SUhIxAAVoZSqKW55UEq3pnDgwIEDBw4cOHDgwIH/CL/qiDHGdF3nPvl8Pj8/P3WkF0IpNdqCGWOqcEmCEKKUsnQL+kallBBiaV4ZoJQSQoqnez0+Y2wmnxEwxpSDSXONMeGUUcZLekYpxRibc63AOQ/XlFJqrdMTt4+TxE5DJ6wb64ndkuMQZJ8wc2YgSSlTr3GJ6ObOeRLCHTO6SXIWHAmYhA1CMxtjGGOZSSPNPidGkQAhZO7FnmsnVKDCXaen/zQd0NIAQAhBukKIfNvPh9ba2xpZfvgWc2Umn6qtTTgeB3edkTWLN0bUNlFh37a+m2qGYXBfNU3j/mvVm1aNRtk8n8/r9ZoY0HVd3/fuq7ZtKaVufYql1Ol0ymRCKZVS3m63iAKMsbZt3Sda68fjgYIBoGma1+sVXdebCACc867rCCH2FSGk73tPYnSuhZQSCYQrR0ZTSoUQQghbXTLGRnO5RygRargUjrHDXBGeGyeXMLhB8VCbxNudngYS0lp78WatxhjjnNfMeP8YZI4MyW3TEngYzWA2S3pZBQAopWHFWhuJzYp5IzHX9cCcPmkWsG0IH45GYeg6G35Vf9ssbjKjwYpaVY0NNPZI7R5DItvU8wBK+sZ++87wGyilObscJlZZXi20CoZhKDjpOOeuJumYWSuxFvBGhHTzj44lUfm7EynlwqFc/8MZvMBcYKFtv/mZJX0nnyt5J8Zavw80TXM+nwHANmv7hVdXuo7aidP+Ikyd0dLNnsRR9puVmQh7n4VtZ2Jk+jZ376jJ8nc1SSthLwpgRYh/L3/uFiCfAd7AltyBAsB654CUEjm9328AuN/vBQeC1vpyuSzOrQTRsM5p3IrL3uURVsWZx9xOddBa52fbHelQnF52pEMxojr8AVNjk66wA2SOAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "cv2_imshow(test_img_model3[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load model"
      ],
      "metadata": {
        "id": "Bm1-1lRafKMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = load_model('/content/gdrive/MyDrive/pre2_models/normalized_dataset_bestfit.h5')\n",
        "model2 = load_model('/content/gdrive/MyDrive/pre2_models/poseestimatorCNN.h5')\n",
        "model3 = load_model('/content/gdrive/MyDrive/pre2_models/edgedetectionCNN.h5')"
      ],
      "metadata": {
        "id": "hF-riy7yfJed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE Average Weght"
      ],
      "metadata": {
        "id": "dsDyFhsCfy9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "kKxth6XwbU0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1_precisions = {\n",
        "\n",
        "#     0: 0.93,\n",
        "#     1: 0.97,\n",
        "#     2: 1.00,\n",
        "#     3: 0.84,\n",
        "#     4: 0.73,\n",
        "#     5: 0.93,\n",
        "#     6: 1.00,\n",
        "#     7: 0.95,\n",
        "#     8: 0.95,\n",
        "#     9: 0.95,\n",
        "#     10: 1.00,\n",
        "#     11: 0.88,\n",
        "#     12: 1.00,\n",
        "#     13: 0.90,\n",
        "#     14: 0.93,\n",
        "#     15: 0.95,\n",
        "#     16: 1.00,\n",
        "#     17: 0.97,\n",
        "#     18: 0.92,\n",
        "#     19: 0.91,\n",
        "#     20: 0.95,\n",
        "#     21: 0.83,\n",
        "#     22: 0.62,\n",
        "#     23: 0.86,\n",
        "#     24: 0.92,\n",
        "#     25: 0.97,\n",
        "#     26: 0.97,\n",
        "#     27: 0.97,\n",
        "#     28: 1.00,\n",
        "#     29: 0.98,\n",
        "#     30: 0.97,\n",
        "#     31: 0.95,\n",
        "#     32: 1.00,\n",
        "#     33: 0.84,\n",
        "#     34: 0.98,\n",
        "#     35: 0.86,\n",
        "#     36: 0.91,\n",
        "#     37: 1.00\n",
        "# }\n",
        "\n",
        "# model2_precisions = {\n",
        "#     0: 0.80,\n",
        "#     1: 0.92,\n",
        "#     2: 0.91,\n",
        "#     3: 0.69,\n",
        "#     4: 0.94,\n",
        "#     5: 0.90,\n",
        "#     6: 0.84,\n",
        "#     7: 0.65,\n",
        "#     8: 0.69,\n",
        "#     9: 0.83,\n",
        "#     10: 0.94,\n",
        "#     11: 0.82,\n",
        "#     12: 0.90,\n",
        "#     13: 0.78,\n",
        "#     14: 0.67,\n",
        "#     15: 0.69,\n",
        "#     16: 0.93,\n",
        "#     17: 1.00,\n",
        "#     18: 0.72,\n",
        "#     19: 0.32,\n",
        "#     20: 0.89,\n",
        "#     21: 0.97,\n",
        "#     22: 0.63,\n",
        "#     23: 0.92,\n",
        "#     24: 0.87,\n",
        "#     25: 0.91,\n",
        "#     26: 1.00,\n",
        "#     27: 0.68,\n",
        "#     28: 0.97,\n",
        "#     29: 0.95,\n",
        "#     30: 0.98,\n",
        "#     31: 1.00,\n",
        "#     32: 0.97,\n",
        "#     33: 0.86,\n",
        "#     34: 0.97,\n",
        "#     35: 0.79,\n",
        "#     36: 0.92,\n",
        "#     37: 0.97\n",
        "\n",
        "# }\n",
        "\n",
        "# model3_precisions = {\n",
        "\n",
        "#     0: 0.79,\n",
        "#     1: 0.90,\n",
        "#     2: 0.89,\n",
        "#     3: 0.66,\n",
        "#     4: 0.60,\n",
        "#     5: 0.82,\n",
        "#     6: 0.81,\n",
        "#     7: 0.85,\n",
        "#     8: 0.90,\n",
        "#     9: 0.80,\n",
        "#     10: 0.95,\n",
        "#     11: 0.89,\n",
        "#     12: 0.81,\n",
        "#     13: 0.75,\n",
        "#     14: 0.80,\n",
        "#     15: 0.88,\n",
        "#     16: 0.88,\n",
        "#     17: 0.88,\n",
        "#     18: 0.76,\n",
        "#     19: 0.90,\n",
        "#     20: 0.95,\n",
        "#     21: 0.90,\n",
        "#     22: 0.67,\n",
        "#     23: 0.89,\n",
        "#     24: 0.91,\n",
        "#     25: 0.89,\n",
        "#     26: 0.90,\n",
        "#     27: 0.95,\n",
        "#     28: 0.97,\n",
        "#     29: 0.90,\n",
        "#     30: 0.97,\n",
        "#     31: 0.95,\n",
        "#     32: 0.87,\n",
        "#     33: 0.84,\n",
        "#     34: 0.97,\n",
        "#     35: 0.80,\n",
        "#     36: 0.92,\n",
        "#     37: 0.97\n",
        "\n",
        "# }"
      ],
      "metadata": {
        "id": "_XEW59LvcRXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision"
      ],
      "metadata": {
        "id": "15IelsarOGbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_precisions = {\n",
        "    0: 1.00,\n",
        "    1: 0.98,\n",
        "    2: 0.99,\n",
        "    3: 0.93,\n",
        "    4: 0.99,\n",
        "    5: 0.98,\n",
        "    6: 0.98,\n",
        "    7: 0.97,\n",
        "    8: 0.98,\n",
        "    9: 0.98,\n",
        "    10: 0.99,\n",
        "    11: 0.99,\n",
        "    12: 0.99,\n",
        "    13: 0.99,\n",
        "    14: 0.98,\n",
        "    15: 0.99,\n",
        "    16: 0.99,\n",
        "    17: 0.99,\n",
        "    18: 0.95,\n",
        "    19: 0.98,\n",
        "    20: 0.98,\n",
        "    21: 1.00,\n",
        "    22: 0.94,\n",
        "    23: 0.95,\n",
        "    24: 0.97,\n",
        "    25: 0.98,\n",
        "    26: 0.98,\n",
        "    27: 0.96,\n",
        "    28: 0.99,\n",
        "    29: 0.99,\n",
        "    30: 0.97,\n",
        "    31: 0.98,\n",
        "    32: 0.95,\n",
        "    33: 0.96,\n",
        "    34: 0.99,\n",
        "    35: 0.96,\n",
        "    36: 0.98,\n",
        "    37: 1.00\n",
        "}\n",
        "model2_precisions = {\n",
        "    0: 0.94,\n",
        "    1: 0.93,\n",
        "    2: 0.96,\n",
        "    3: 0.88,\n",
        "    4: 0.96,\n",
        "    5: 0.86,\n",
        "    6: 0.84,\n",
        "    7: 0.83,\n",
        "    8: 0.87,\n",
        "    9: 0.94,\n",
        "    10: 0.97,\n",
        "    11: 0.93,\n",
        "    12: 0.96,\n",
        "    13: 0.90,\n",
        "    14: 0.83,\n",
        "    15: 0.88,\n",
        "    16: 1.00,\n",
        "    17: 0.99,\n",
        "    18: 0.85,\n",
        "    19: 0.95,\n",
        "    20: 0.97,\n",
        "    21: 1.00,\n",
        "    22: 0.85,\n",
        "    23: 0.94,\n",
        "    24: 0.96,\n",
        "    25: 0.99,\n",
        "    26: 0.99,\n",
        "    27: 0.93,\n",
        "    28: 0.99,\n",
        "    29: 0.97,\n",
        "    30: 0.97,\n",
        "    31: 0.99,\n",
        "    32: 0.95,\n",
        "    33: 0.93,\n",
        "    34: 0.98,\n",
        "    35: 0.95,\n",
        "    36: 0.93,\n",
        "    37: 0.92\n",
        "}\n",
        "model3_precisions = {\n",
        "    0: 0.98,\n",
        "    1: 0.96,\n",
        "    2: 0.98,\n",
        "    3: 0.96,\n",
        "    4: 0.95,\n",
        "    5: 0.96,\n",
        "    6: 0.96,\n",
        "    7: 0.96,\n",
        "    8: 0.98,\n",
        "    9: 0.97,\n",
        "    10: 0.96,\n",
        "    11: 0.99,\n",
        "    12: 0.94,\n",
        "    13: 0.98,\n",
        "    14: 0.97,\n",
        "    15: 0.98,\n",
        "    16: 0.97,\n",
        "    17: 0.95,\n",
        "    18: 0.95,\n",
        "    19: 0.96,\n",
        "    20: 0.98,\n",
        "    21: 0.97,\n",
        "    22: 0.94,\n",
        "    23: 0.93,\n",
        "    24: 0.95,\n",
        "    25: 0.99,\n",
        "    26: 0.96,\n",
        "    27: 1.00,\n",
        "    28: 0.99,\n",
        "    29: 0.99,\n",
        "    30: 0.96,\n",
        "    31: 0.99,\n",
        "    32: 0.98,\n",
        "    33: 0.96,\n",
        "    34: 0.98,\n",
        "    35: 0.97,\n",
        "    36: 0.98,\n",
        "    37: 0.93\n",
        "}"
      ],
      "metadata": {
        "id": "5CyMijHlOF3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Predictions"
      ],
      "metadata": {
        "id": "q0U6hYsyf1GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Initialize empty arrays to store predictions\n",
        "predictions_model1 = np.zeros_like(test_label_model1)\n",
        "predictions_model2 = np.zeros_like(test_label_model1)\n",
        "predictions_model3 = np.zeros_like(test_label_model1)\n",
        "\n",
        "# Make predictions using each base model\n",
        "predictions_model1 = model1.predict(test_img_model1)\n",
        "predictions_model2 = model2.predict(test_img_model2)\n",
        "predictions_model3 = model3.predict(test_img_model3)\n",
        "\n",
        "# Initialize ensemble predictions array\n",
        "ensemble_predictions = np.zeros((len(test_img_model3), 38))  # Updated initialization\n",
        "\n",
        "\n",
        "# Iterate over each sample and calculate ensemble predictions\n",
        "for i in range(len(test_label_model1)):\n",
        "    # Get the predicted class for each model\n",
        "    predicted_class_model1 = np.argmax(predictions_model1[i])\n",
        "    predicted_class_model2 = np.argmax(predictions_model2[i])\n",
        "    predicted_class_model3 = np.argmax(predictions_model3[i])\n",
        "\n",
        "    # Retrieve the precision values based on the predicted classes\n",
        "    precision_model1 = model1_precisions.get(predicted_class_model1)\n",
        "    precision_model2 = model2_precisions.get(predicted_class_model2)\n",
        "    precision_model3 = model3_precisions.get(predicted_class_model3)\n",
        "\n",
        "    # Calculate weights based on precision values\n",
        "    weight_model1 = precision_model1\n",
        "    weight_model2 = precision_model2\n",
        "    weight_model3 = precision_model3\n",
        "\n",
        "    # Calculate ensemble prediction by weighted averaging\n",
        "    ensemble_predictions[i] = (weight_model1 * predictions_model1[i] +\n",
        "                               weight_model2 * predictions_model2[i] +\n",
        "                               weight_model3 * predictions_model3[i]) / 3.0\n",
        "\n",
        "# Evaluate ensemble predictions\n",
        "ensemble_accuracy = accuracy_score(test_label_model1, ensemble_predictions.argmax(axis=1))\n",
        "\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(test_label_model1, predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(test_label_model1, predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(test_label_model1, predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Dynamic Ensemble Accuracy:\", ensemble_accuracy)"
      ],
      "metadata": {
        "id": "VLTZ8FY3gC_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63da844-f583-46fa-ad07-87bddba58c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 1s 6ms/step\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "48/48 [==============================] - 1s 8ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8184210526315789\n",
            "Base Model 3 Accuracy: 0.8585526315789473\n",
            "Dynamic Ensemble Accuracy: 0.9480263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize empty arrays to store predictions\n",
        "predictions_model1 = np.zeros_like(test_label_model1)\n",
        "predictions_model2 = np.zeros_like(test_label_model1)\n",
        "predictions_model3 = np.zeros_like(test_label_model1)\n",
        "\n",
        "# Make predictions using each base model\n",
        "predictions_model1 = model1.predict(test_img_model1)\n",
        "predictions_model2 = model2.predict(test_img_model2)\n",
        "predictions_model3 = model3.predict(test_img_model3)\n",
        "\n",
        "# Initialize ensemble predictions array\n",
        "ensemble_predictions = np.zeros((len(test_img_model3), 38))\n",
        "\n",
        "# Iterate over each sample and calculate ensemble predictions\n",
        "for i in range(len(test_label_model1)):\n",
        "    # Get the predicted class for each model\n",
        "    predicted_class_model1 = np.argmax(predictions_model1[i])\n",
        "    predicted_class_model2 = np.argmax(predictions_model2[i])\n",
        "    predicted_class_model3 = np.argmax(predictions_model3[i])\n",
        "\n",
        "    # Calculate ensemble prediction by simple averaging\n",
        "    ensemble_predictions[i] = (predictions_model1[i] +\n",
        "                               predictions_model2[i] +\n",
        "                               predictions_model3[i]) / 3.0\n",
        "\n",
        "# Evaluate ensemble predictions\n",
        "ensemble_accuracy = accuracy_score(test_label_model1, ensemble_predictions.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(test_label_model1, predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(test_label_model1, predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(test_label_model1, predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Unweighted Ensemble Accuracy:\", ensemble_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fd53oJd1At",
        "outputId": "a1a162ed-0cbb-41a5-c895-353db8441c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 0s 5ms/step\n",
            "48/48 [==============================] - 0s 7ms/step\n",
            "48/48 [==============================] - 0s 7ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8184210526315789\n",
            "Base Model 3 Accuracy: 0.8585526315789473\n",
            "Unweighted Ensemble Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize empty arrays to store predictions\n",
        "predictions_model1 = np.zeros_like(test_label_model1)\n",
        "predictions_model2 = np.zeros_like(test_label_model1)\n",
        "predictions_model3 = np.zeros_like(test_label_model1)\n",
        "\n",
        "# Make predictions using each base model\n",
        "predictions_model1 = model1.predict(test_img_model1)\n",
        "predictions_model2 = model2.predict(test_img_model2)\n",
        "predictions_model3 = model3.predict(test_img_model3)\n",
        "\n",
        "# Initialize ensemble predictions array\n",
        "ensemble_predictions = np.zeros((len(test_img_model3), 38))\n",
        "\n",
        "# Define static weights for each model\n",
        "weight_model1 = 0.98\n",
        "weight_model2 = 0.93\n",
        "weight_model3 = 0.97\n",
        "\n",
        "# Iterate over each sample and calculate ensemble predictions\n",
        "for i in range(len(test_label_model1)):\n",
        "    # Get the predicted class for each model\n",
        "    predicted_class_model1 = np.argmax(predictions_model1[i])\n",
        "    predicted_class_model2 = np.argmax(predictions_model2[i])\n",
        "    predicted_class_model3 = np.argmax(predictions_model3[i])\n",
        "\n",
        "    # Calculate ensemble prediction using static weights\n",
        "    ensemble_predictions[i] = (weight_model1 * predictions_model1[i] +\n",
        "                               weight_model2 * predictions_model2[i] +\n",
        "                               weight_model3 * predictions_model3[i]) / (weight_model1 + weight_model2 + weight_model3)\n",
        "\n",
        "# Evaluate ensemble predictions\n",
        "ensemble_accuracy = accuracy_score(test_label_model1, ensemble_predictions.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(test_label_model1, predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(test_label_model1, predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(test_label_model1, predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Static Weight Ensemble Accuracy:\", ensemble_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oL0iCjkawT",
        "outputId": "3445f833-72cd-41ea-92e4-433e1df98cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 0s 9ms/step\n",
            "48/48 [==============================] - 1s 10ms/step\n",
            "48/48 [==============================] - 0s 8ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8184210526315789\n",
            "Base Model 3 Accuracy: 0.8585526315789473\n",
            "Static Weight Ensemble Accuracy: 0.9513157894736842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#advance meta learner"
      ],
      "metadata": {
        "id": "FWGvyW9pl4Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "num_classes = 38\n",
        "test_label_model1_one_hot = to_categorical(test_label_model1, num_classes)\n",
        "test_label_model2_one_hot = to_categorical(test_label_model2, num_classes)\n",
        "test_label_model3_one_hot = to_categorical(test_label_model3, num_classes)\n",
        "\n",
        "# Split the data into training and validation sets for each base model\n",
        "X_train_model1, X_val_model1, y_train_model1, y_val_model1 = train_test_split(test_img_model1, test_label_model1_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model2, X_val_model2, y_train_model2, y_val_model2 = train_test_split(test_img_model2, test_label_model2_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model3, X_val_model3, y_train_model3, y_val_model3 = train_test_split(test_img_model3, test_label_model3_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions on the validation sets for each base model\n",
        "val_predictions_model1 = model1.predict(X_val_model1)\n",
        "val_predictions_model2 = model2.predict(X_val_model2)\n",
        "val_predictions_model3 = model3.predict(X_val_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_val_predictions_model1 = val_predictions_model1 * weight_model1\n",
        "weighted_val_predictions_model2 = val_predictions_model2 * weight_model2\n",
        "weighted_val_predictions_model3 = val_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the validation dataset for the meta-learner\n",
        "meta_val_data = np.column_stack((weighted_val_predictions_model1, weighted_val_predictions_model2, weighted_val_predictions_model3))\n",
        "\n",
        "# Train the meta-learner model\n",
        "meta_learner = RandomForestClassifier()\n",
        "meta_learner.fit(meta_val_data, y_val_model1.argmax(axis=1))  # Convert one-hot encoded labels to categorical labels\n",
        "\n",
        "# Make predictions using each base model on the test set\n",
        "test_predictions_model1 = model1.predict(X_train_model1)\n",
        "test_predictions_model2 = model2.predict(X_train_model2)\n",
        "test_predictions_model3 = model3.predict(X_train_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_test_predictions_model1 = test_predictions_model1 * weight_model1\n",
        "weighted_test_predictions_model2 = test_predictions_model2 * weight_model2\n",
        "weighted_test_predictions_model3 = test_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the test dataset for the meta-learner\n",
        "meta_test_data = np.column_stack((weighted_test_predictions_model1, weighted_test_predictions_model2, weighted_test_predictions_model3))\n",
        "\n",
        "# Make predictions using the meta-learner\n",
        "meta_learner_predictions = meta_learner.predict(meta_test_data)\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy of the meta-learner\n",
        "meta_learner_accuracy = accuracy_score(y_train_model1.argmax(axis=1), meta_learner_predictions)\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Wegihted RandomForest Meta-Learner Accuracy:\", meta_learner_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHPlE1CKv_V_",
        "outputId": "5d1c0ff6-f799-4508-947b-1701621ce08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 5ms/step\n",
            "10/10 [==============================] - 0s 8ms/step\n",
            "10/10 [==============================] - 0s 9ms/step\n",
            "38/38 [==============================] - 0s 6ms/step\n",
            "38/38 [==============================] - 0s 8ms/step\n",
            "38/38 [==============================] - 0s 10ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8157894736842105\n",
            "Base Model 3 Accuracy: 0.850328947368421\n",
            "Wegihted RandomForest Meta-Learner Accuracy: 0.9136513157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "num_classes = 38\n",
        "test_label_model1_one_hot = to_categorical(test_label_model1, num_classes)\n",
        "test_label_model2_one_hot = to_categorical(test_label_model2, num_classes)\n",
        "test_label_model3_one_hot = to_categorical(test_label_model3, num_classes)\n",
        "\n",
        "# Split the data into training and validation sets for each base model\n",
        "X_train_model1, X_val_model1, y_train_model1, y_val_model1 = train_test_split(test_img_model1, test_label_model1_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model2, X_val_model2, y_train_model2, y_val_model2 = train_test_split(test_img_model2, test_label_model2_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model3, X_val_model3, y_train_model3, y_val_model3 = train_test_split(test_img_model3, test_label_model3_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions on the validation sets for each base model\n",
        "val_predictions_model1 = model1.predict(X_val_model1)\n",
        "val_predictions_model2 = model2.predict(X_val_model2)\n",
        "val_predictions_model3 = model3.predict(X_val_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_val_predictions_model1 = val_predictions_model1 * weight_model1\n",
        "weighted_val_predictions_model2 = val_predictions_model2 * weight_model2\n",
        "weighted_val_predictions_model3 = val_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the validation dataset for the meta-learner\n",
        "meta_val_data = np.column_stack((weighted_val_predictions_model1, weighted_val_predictions_model2, weighted_val_predictions_model3))\n",
        "\n",
        "# Train the meta-learner model\n",
        "meta_learner = SVC()\n",
        "meta_learner.fit(meta_val_data, y_val_model1.argmax(axis=1))  # Convert one-hot encoded labels to categorical labels\n",
        "\n",
        "# Make predictions using each base model on the test set\n",
        "test_predictions_model1 = model1.predict(X_train_model1)\n",
        "test_predictions_model2 = model2.predict(X_train_model2)\n",
        "test_predictions_model3 = model3.predict(X_train_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_test_predictions_model1 = test_predictions_model1 * weight_model1\n",
        "weighted_test_predictions_model2 = test_predictions_model2 * weight_model2\n",
        "weighted_test_predictions_model3 = test_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the test dataset for the meta-learner\n",
        "meta_test_data = np.column_stack((weighted_test_predictions_model1, weighted_test_predictions_model2, weighted_test_predictions_model3))\n",
        "\n",
        "# Make predictions using the meta-learner\n",
        "meta_learner_predictions = meta_learner.predict(meta_test_data)\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy of the meta-learner\n",
        "meta_learner_accuracy = accuracy_score(y_train_model1.argmax(axis=1), meta_learner_predictions)\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Weighted SVM Meta-Learner Accuracy:\", meta_learner_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WINoff29wnZ_",
        "outputId": "4203e333-33e2-47d8-a177-81cfad77d92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 14ms/step\n",
            "10/10 [==============================] - 0s 18ms/step\n",
            "10/10 [==============================] - 0s 15ms/step\n",
            "38/38 [==============================] - 1s 12ms/step\n",
            "38/38 [==============================] - 1s 16ms/step\n",
            "38/38 [==============================] - 1s 13ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8157894736842105\n",
            "Base Model 3 Accuracy: 0.850328947368421\n",
            "Weighted SVM Meta-Learner Accuracy: 0.9194078947368421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "num_classes = 38\n",
        "test_label_model1_one_hot = to_categorical(test_label_model1, num_classes)\n",
        "test_label_model2_one_hot = to_categorical(test_label_model2, num_classes)\n",
        "test_label_model3_one_hot = to_categorical(test_label_model3, num_classes)\n",
        "\n",
        "# Split the data into training and validation sets for each base model\n",
        "X_train_model1, X_val_model1, y_train_model1, y_val_model1 = train_test_split(test_img_model1, test_label_model1_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model2, X_val_model2, y_train_model2, y_val_model2 = train_test_split(test_img_model2, test_label_model2_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model3, X_val_model3, y_train_model3, y_val_model3 = train_test_split(test_img_model3, test_label_model3_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions on the validation sets for each base model\n",
        "val_predictions_model1 = model1.predict(X_val_model1)\n",
        "val_predictions_model2 = model2.predict(X_val_model2)\n",
        "val_predictions_model3 = model3.predict(X_val_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_val_predictions_model1 = val_predictions_model1 * weight_model1\n",
        "weighted_val_predictions_model2 = val_predictions_model2 * weight_model2\n",
        "weighted_val_predictions_model3 = val_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the validation dataset for the meta-learner\n",
        "meta_val_data = np.column_stack((weighted_val_predictions_model1, weighted_val_predictions_model2, weighted_val_predictions_model3))\n",
        "\n",
        "# Train the meta-learner model\n",
        "meta_learner = KNeighborsClassifier()\n",
        "meta_learner.fit(meta_val_data, y_val_model1.argmax(axis=1))  # Convert one-hot encoded labels to categorical labels\n",
        "\n",
        "# Make predictions using each base model on the test set\n",
        "test_predictions_model1 = model1.predict(X_train_model1)\n",
        "test_predictions_model2 = model2.predict(X_train_model2)\n",
        "test_predictions_model3 = model3.predict(X_train_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_test_predictions_model1 = test_predictions_model1 * weight_model1\n",
        "weighted_test_predictions_model2 = test_predictions_model2 * weight_model2\n",
        "weighted_test_predictions_model3 = test_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the test dataset for the meta-learner\n",
        "meta_test_data = np.column_stack((weighted_test_predictions_model1, weighted_test_predictions_model2, weighted_test_predictions_model3))\n",
        "\n",
        "# Make predictions using the meta-learner\n",
        "meta_learner_predictions = meta_learner.predict(meta_test_data)\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy of the meta-learner\n",
        "meta_learner_accuracy = accuracy_score(y_train_model1.argmax(axis=1), meta_learner_predictions)\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Weighted KNN Meta-Learner Accuracy:\", meta_learner_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSzU5N9tw_0m",
        "outputId": "4a71ba49-fc2d-42b4-9384-7a5ab08f5730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 7ms/step\n",
            "10/10 [==============================] - 0s 11ms/step\n",
            "10/10 [==============================] - 0s 14ms/step\n",
            "38/38 [==============================] - 0s 8ms/step\n",
            "38/38 [==============================] - 1s 15ms/step\n",
            "38/38 [==============================] - 0s 12ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8157894736842105\n",
            "Base Model 3 Accuracy: 0.850328947368421\n",
            "Weighted KNN Meta-Learner Accuracy: 0.9235197368421053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "num_classes = 38\n",
        "test_label_model1_one_hot = to_categorical(test_label_model1, num_classes)\n",
        "test_label_model2_one_hot = to_categorical(test_label_model2, num_classes)\n",
        "test_label_model3_one_hot = to_categorical(test_label_model3, num_classes)\n",
        "\n",
        "# Split the data into training and validation sets for each base model\n",
        "X_train_model1, X_val_model1, y_train_model1, y_val_model1 = train_test_split(test_img_model1, test_label_model1_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model2, X_val_model2, y_train_model2, y_val_model2 = train_test_split(test_img_model2, test_label_model2_one_hot, test_size=0.2, random_state=42)\n",
        "X_train_model3, X_val_model3, y_train_model3, y_val_model3 = train_test_split(test_img_model3, test_label_model3_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions on the validation sets for each base model\n",
        "val_predictions_model1 = model1.predict(X_val_model1)\n",
        "val_predictions_model2 = model2.predict(X_val_model2)\n",
        "val_predictions_model3 = model3.predict(X_val_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_val_predictions_model1 = val_predictions_model1 * weight_model1\n",
        "weighted_val_predictions_model2 = val_predictions_model2 * weight_model2\n",
        "weighted_val_predictions_model3 = val_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the validation dataset for the meta-learner\n",
        "meta_val_data = np.column_stack((weighted_val_predictions_model1, weighted_val_predictions_model2, weighted_val_predictions_model3))\n",
        "\n",
        "# Train the meta-learner model (Logistic Regression with one-vs-rest)\n",
        "meta_learner = LogisticRegression(multi_class='ovr')\n",
        "meta_learner.fit(meta_val_data, y_val_model1.argmax(axis=1))  # Convert one-hot encoded labels to categorical labels\n",
        "\n",
        "# Make predictions using each base model on the test set\n",
        "test_predictions_model1 = model1.predict(X_train_model1)\n",
        "test_predictions_model2 = model2.predict(X_train_model2)\n",
        "test_predictions_model3 = model3.predict(X_train_model3)\n",
        "\n",
        "# Apply weights to the predictions of each base model\n",
        "weighted_test_predictions_model1 = test_predictions_model1 * weight_model1\n",
        "weighted_test_predictions_model2 = test_predictions_model2 * weight_model2\n",
        "weighted_test_predictions_model3 = test_predictions_model3 * weight_model3\n",
        "\n",
        "# Create the test dataset for the meta-learner\n",
        "meta_test_data = np.column_stack((weighted_test_predictions_model1, weighted_test_predictions_model2, weighted_test_predictions_model3))\n",
        "\n",
        "# Make predictions using the meta-learner\n",
        "meta_learner_predictions = meta_learner.predict(meta_test_data)\n",
        "\n",
        "# Calculate accuracy for each base model\n",
        "accuracy_model1 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model1.argmax(axis=1))\n",
        "accuracy_model2 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model2.argmax(axis=1))\n",
        "accuracy_model3 = accuracy_score(y_train_model1.argmax(axis=1), test_predictions_model3.argmax(axis=1))\n",
        "\n",
        "# Calculate accuracy of the meta-learner\n",
        "meta_learner_accuracy = accuracy_score(y_train_model1.argmax(axis=1), meta_learner_predictions)\n",
        "\n",
        "# Print accuracy of each base model\n",
        "print(\"Base Model 1 Accuracy:\", accuracy_model1)\n",
        "print(\"Base Model 2 Accuracy:\", accuracy_model2)\n",
        "print(\"Base Model 3 Accuracy:\", accuracy_model3)\n",
        "\n",
        "print(\"Weighted Logistic Regression Meta-Learner Accuracy:\", meta_learner_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po7aoSxkxwtu",
        "outputId": "fc04ce21-2bcb-4ec3-d348-2cff99cc6a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 8ms/step\n",
            "10/10 [==============================] - 0s 10ms/step\n",
            "10/10 [==============================] - 0s 11ms/step\n",
            "38/38 [==============================] - 1s 13ms/step\n",
            "38/38 [==============================] - 1s 12ms/step\n",
            "38/38 [==============================] - 0s 8ms/step\n",
            "Base Model 1 Accuracy: 0.9243421052631579\n",
            "Base Model 2 Accuracy: 0.8157894736842105\n",
            "Base Model 3 Accuracy: 0.850328947368421\n",
            "Weighted Logistic Regression Meta-Learner Accuracy: 0.9366776315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy: 0.9473684210526315"
      ],
      "metadata": {
        "id": "vw1NUZ_XzQVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}